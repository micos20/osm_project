{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Open Street Map data  <a id=\"wrangle-open-street-map-data\"></a>\n",
    "\n",
    "__Project documentation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text documents the investigations performed on Open Street Map (OSM) raw data extracted from [Overpass query](https://overpass-api.de/query_form.html). We will screen and audit the raw data, perform some cleaning operations and create a SQL database for further investigation of the OSM content. The following points are addressed.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table-of-content\"></a> \n",
    "<p style=\"font-size: large\"> Table of content</p> \n",
    "\n",
    "1. [Investigated area](#investigated-area)  \n",
    "2. [Auditing OSM raw data](#auditing-osm-raw-data)  \n",
    " a) [Audit node and way attributes](#audit-node-and-way-attributes)  \n",
    " b) [Audit node and way tags](#audit-node-and-way-tags)  \n",
    " c) [Audit way nodes references](#audit-way-nodes-references)  \n",
    "3. [Cleaning operations](#cleaning-operations)   \n",
    " a) [Clean weblinks](#clean-weblinks)  \n",
    " b) [Clean phone numbers](#clean-phone-numbers)  \n",
    "4. [SQL Database](#sql-database)  \n",
    " a) [Database creation](#database-creation)  \n",
    " b) [Database queries](#database-queries)   \n",
    "5. [Summary](#summary)   \n",
    "6. [List of references](#list-of-references)    \n",
    "7. [List of files](#list-of-files)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigated area <a id=\"investigated-area\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The investigated region is approximately 35km north of Hamburg, Germany. The [image](#InvestigatedArea_600x474) below shows the extracted region. I selected this area because I've been living here for more than 20 year. I'm actually keen to know how many information and details I are already implemented in the OSM map.\n",
    "\n",
    "The following overpass query was used to extract the raw data in XML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "overpass query\n",
    "(\n",
    "   node(53.6782,9.6072,53.7988,9.7888);\n",
    "   <;\n",
    ");\n",
    "out meta;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OSM Area: Germany, Schleswig-Hostein, Pinneberg, Elmshorn & Uetersen](https://github.com/micos20/osm_project/blob/master/images/InvestigatedArea_600x474.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/micos20/osm_project/blob/master/images/InvestigatedArea_600x474.PNG\" width = \"600\" height=\"474\" alt=\"OSM Area: Germany, Schleswig-Hostein, Pinneberg, Elmshorn & Uetersen\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"InvestigatedArea_600x474\"></a>  \n",
    "![Investigated area: Germany, Schleswig-Hostein, Pinneberg, Elmshorn & Uetersen](./images/InvestigatedArea_600x474.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure><figcaption style=\"text-indent:10%\">Investigated area: Germany, Schleswig-Hostein, Pinneberg, Elmshorn & Uetersen</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"font-size: small; font-style: italic\" href=#table-of-content>Back to table of content.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing OSM raw data  <a id=\"auditing-osm-raw-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview of the data I first checked some general things regarding the OSM content. I used the the script [*quick_parse_osm.py*](#list-of-files) for this purpose. \n",
    "The xml-file contains the following tags:\n",
    "\n",
    "<p style=\"text-indent: 20%; font-weight: bold; font-size: small\">Overview of tags in OSM raw data:</p>\n",
    "\n",
    "Tag name | Counts | sub-tags| ref nodes/ members \n",
    "--|--|--|--\n",
    "osm| 1||\n",
    "note| 1||\n",
    "meta| 1||\n",
    "node| 252825| 46546|\n",
    "way| 51297| 176900| 346424\n",
    "relation| 614 | 2902| 39735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Audit node and way attributes <a id=\"audit-node-and-way-attributes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python script [*audit_nodes.py*](#list-of-files) is used to audit the node attributes, the script [*audit_ways.py*](#list-of-files) is used to audit way attributes. All attributes are checked for correct data type. All integer types and *datestamp* are checked for range (min, max) and validity (e.g. valid date, all values positive). The nodal coordinates *lat*, *lon* are also checked for correct position (within defined bounding box from overpass query). *Users* are checked for problematic characters.  \n",
    "\n",
    "All fields show valid data types. The node *id*s range from 131499 to 7869305206, the way *id*s from 4043904 to 842937254.\n",
    "The coordinates of all nodes are within the bounding box of our investigated region. \n",
    "The *uid* for nodes and ways ranges between 50 to 11558570. There are 468 unique users in node tags and 444 users in way tags. There are three user names showing problematic characters.  \n",
    "<p style=\"text-indent: 20%; font-weight: bold; font-size: small\">Problematic user names:</p>\n",
    "\n",
    "*uid* | *user* | node/way\n",
    "--|--|--\n",
    "6526984 | @mmanuel | node\n",
    "1041363 | nit@bse | node and way\n",
    "45059 | &lt;don&gt; | way\n",
    "\n",
    "The earliest recorded change made to nodes in this data set is `2007-09-10 08:58:41+00:00` and the latest `2020-09-02 18:21:42+00:00`. Way *timestamp*s lay within this range.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit node and way tags <a id=\"audit-node-and-way-tags\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All *tag*s consist of a key/ value pair (*k*/ *v*). The script [*audit_tags.py*](#list-of-files) is used to assess the data. All key/ value pairs are checked for problematic characters.\n",
    "\n",
    "#### Audit tag keys\n",
    "\n",
    "The keys in the xml file are colon separated keys like `railway:ref:DBAG`. I use the first part of the key as *type* and the remaining string as *key*. This results for the above mentioned key in: `type=railway` and `key=ref:DBA`. If there is no colon in the key string the *type* is set to 'regular'.\n",
    "\n",
    "There are 787 different type/ key combinations in the OSM data including 111 unique *type*s. Each *type* has several different *key*s. The list below shows the most prominent *type*s and some exemplary *key* names.\n",
    "\n",
    "<p style=\"text-indent: 20%; font-weight: bold; font-size: small\">Overview of <em>type/ key</em> combinations including example <em>key</em>s:</p>\n",
    "\n",
    "Type | n° of keys | Example keys\n",
    "--|--|--\n",
    "regular | 320 | 'meter_load', 'loc_name', 'landuse'\n",
    "railway | 72 | 'signal:speed_limit:speed', 'signal:station_distant', 'position'\n",
    "destination | 33 | 'colour:to:forward', 'colour:backward', 'colour:text'\n",
    "recycling | 22 | 'plastic_packaging', 'plastic_bottles', 'waste'\n",
    "openGeoDB | 16 | 'version', 'type', 'auto_update'\n",
    "note | 16 | 'stripclub', 'de', 'vacant'\n",
    "payment | 15 | 'debit_cards', 'ep_geldkarte', 'girocard'\n",
    "removed | 14 | 'internet_access:fee', 'landuse', 'website'\n",
    "socket | 14 | 'schuko:output', 'type2:voltage', 'type2_combo:voltage'\n",
    "fuel | 12 | 'GTL_diesel', 'octane_102', 'biogas'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no problematic characters in the type/ key names. Some keys use capital letters or numbers (e.g. `openGeoDB:telephone_area_code`, but this doen't seem to be a problem. \n",
    "\n",
    "As the keys are composed of different colon separated names the very same name could occure in different type/ key combinations. If we were to search for websites for example we need to consult at least three different *type*s. The `key=website` can be found in the types *regular*, *contact* and *removed*. In addition websites can be found in *url* keys. Or we might want to look for shops. The `key=shop` can be found in type *regular*, *ref*, *note* and *disused*. We need to consider this fact when we clean the data and query the SQL database later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit tag values  \n",
    "There are no missing or *NULL* values in the data set. I've found problematic characters for *value* attributes in 86 keys. The table below shows the *key*s (first 10) containing the most problematic *value*s.\n",
    "\n",
    "<p style=\"text-indent: 20%; font-weight: bold; font-size: small\">Number of problematic <em>value</em>s found in <em>key</em>s:</p>\n",
    "\n",
    "*key* | problematic *value*s  \n",
    "--|-- \n",
    "website | 450\n",
    "name | 359  \n",
    "opening_hours | 191   \n",
    "phone | 189\n",
    "ref | 138\n",
    "email | 79  \n",
    "fax | 64 \n",
    "note | 43\n",
    "backward | 38\n",
    "operator | 37\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *name* values contain predominantly bus stops or railway items like crossings as can be seen in the output below. I show only an extract of the output of *name* values starting with 'B'. \n",
    "```python\n",
    "print(\"Checking problematic name values starting with 'B':\")\n",
    "for name in (n for n in pbl_values['name'] if n[0] == 'B'):\n",
    "    print(name)\n",
    "```\n",
    "    ...\n",
    "    Barmstedt, Markt\n",
    "    Boje-C.-Steffen-Gemeinschaftsschule Elmshorn\n",
    "    Barmstedt, Baumschulenweg\n",
    "    Bevern, Steinfurth\n",
    "    Barmstedt, Galgenberg\n",
    "    Bevern, Schmiedekamp\n",
    "    Bevern, Tannenweg\n",
    "    Bf. Tornesch\n",
    "    Bullenkuhlen, Schulweg\n",
    "    Brücke Elmshorn e.V.\n",
    "    Bf. Prisdorf (SEV)\n",
    "    Bevern, Barkhörner Weg\n",
    "    Barmstedt, Hamburger Straße\n",
    "    Bi'n Himmel\n",
    "    Bullenkuhlen, Achterstraße\n",
    "    Bullenkuhlen, Seether Weg (Mitte)\n",
    "    Bergmann & Söhne\n",
    "    Bullendorf, Feuerwehrhaus\n",
    "    Blumen & Gestaltung Sudeck\n",
    "    Bevern, Am Gehölz\n",
    "    Bf. Elmshorn (ZOB)\n",
    "    Bob's Teeregal\n",
    "    Barmstedt, Chemnitzstraße\n",
    "    Berufliche Schule Elmshorn, Europaschule\n",
    "    Barmstedt, Gymnasium\n",
    "    Bf. Elmshorn (Holstenplatz)\n",
    "    BÜ 27 \"Wrangelpromenade\"\n",
    "    BÜ 29 \"Grenzweg\"\n",
    "    BÜ 28 \"Gerlingweg\"\n",
    "    Bevern, Holstein\n",
    "    B+K Wohnkultur u. Boge/Clasen \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *name* values contain lots of different special characters like `'`, `&`, `\"` and `()`. Some of the street names are quoted like `BÜ 27 \"Wrangelpromenade\"`. The raw xml data for the latter example is given below.\n",
    "```xml\n",
    " <node id=\"1239947780\" lat=\"53.7679521\" lon=\"9.6554377\" version=\"5\" timestamp=\"2019-01-18T09:46:14Z\" changeset=\"66420457\" uid=\"677977\" user=\"peter_elveshorn\">\n",
    "    <tag k=\"crossing:barrier\" v=\"no\"/>\n",
    "    <tag k=\"name\" v=\"BÜ 27 &quot;Wrangelpromenade&quot;\"/>\n",
    "    <tag k=\"railway\" v=\"level_crossing\"/>\n",
    "    <tag k=\"source\" v=\"Bing\"/> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python correctly interprets html metacharaters and translates `&quot;` to `\"`. This *value* is later exported into a csv file as: \n",
    "\n",
    "`1239947780,name,\"BÜ 27 \"\"Wrangelpromenade\"\"\",regular`.   \n",
    "\n",
    "Double quotes are used by default to escape meta characters by the python csv module. The subsequent SQL import works also perfectly fine as we'll see later. The other meta characters like `&gt;`, `&lt;` and `&amp;` are translated correctly as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites by default contain special characters like `:` or `/`. A quick glance at the html links reveals that the links are given in different formats as the following list demonstartes. \n",
    "\n",
    "    https://www.vb-piel.de/  \n",
    "    https://fitness-barmstedt.de  \n",
    "    www.buongiorno-caffe.de  \n",
    "    https://www.schnickschnack-shop.de/  \n",
    "    http://www.bruecke-sh.de/index.php?idm=132  \n",
    "    https://www.hanssen-for-men.de/  \n",
    "    http://www.studienkreis.de/elmshorn.html  \n",
    "    https://www.maass24.de  \n",
    "    http://www.auszeit-elmshorn.net/  \n",
    "    www.nur-hier.de\n",
    "    https://www.mcdonalds.de/restaurant?url=elmshorn-lise-meitner-str-1&/de\n",
    "    gefluegelhof-neumann.de\n",
    " \n",
    "Some http links omit the `http://` part. Some use secure `https` some not. Many websites use `www`, others not. And some links even integrate http queries. But the most important question is still not answered. Does the link still work or is it broken, has the url moved elsewhere or is it insecure?\n",
    "\n",
    "I decided to clarify these questions and refer to chapter [Clean weblinks](#clean-weblinks) where we will discuss weblinks in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With telephone number there is an almost similiar issue. The numbers are given in many different formats as can be seen in the list below.\n",
    "\n",
    "Telephone numbers showing different formats:\n",
    "\n",
    "    +49 4122-9994713\n",
    "    +49 4121 91213\n",
    "    +49 41212611779\n",
    "    +49 (4123) 92 17 93\n",
    "    +49/4121/21773\n",
    "    +49 4121 643-0\n",
    "    +49 4123 9290577;+49 4123 9222240\n",
    "    +494121750205\n",
    "\n",
    "Some numbers divide country code and area codes by white spaces others by `/` or `-`. And sometimes the phone number doesn't contain any delimiters at all as you can see in the last number of the above list. Often the local code is given in parenthesis as this is widely used in Germany. It also happens that the *phone* value contains more than one number. In that case the numbers are separated by `;`. Open street map allows this behavior as stated on [OSM Key:phone wiki](https://wiki.openstreetmap.org/wiki/Key%3Aphone) (see *Parsing phone numbers*).\n",
    "\n",
    "The cleaning of the phone numbers is described in chapter [Clean phone numbers](#clean-phone-numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above addresses can be found in the *regular* keys *ref* or *name* referring to bus stops, street cabinets, post boxes and railway items. Generally the key *addr:...* is used to hold address data like street names, post codes, city, country, housenumber, etc.    \n",
    "\n",
    "I checked *street* and *postcode* keys for abbreviations and problematic characters. There are 858 street names and 12 valid post codes in the node and way tags. There is not a single abbreviation for `Straße` (street) within the *value*s. The address data looks actually pretty clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit way nodes references <a id=\"audit-way-nodes-references\"></a>\n",
    "\n",
    "There is no way w/o node references in the data set. All types are okay. 596 ways reference to 6044 nodes which aren't in the data set. These way nodes are outside our investigated region (bounding box).\n",
    "The function *write_dummy_noodes* (from *audit_ways.py*) is used to create 6044 dummy nodes written to a csv file which we'll import to our SQL database later. The nodes are witten to the file [dummy_nodes.csv](#list-of-files). In addition a node tag for all dummy nodes is written to [dummy_nodes_tags.csv](#list-of-files) containing a *note* key to explain why this node has been created. The *uid* is set to -1 and the *user* to 'dummy'. The coordinates are set to 0.0/0.0 (lon/lat).\n",
    "\n",
    "First 4 rows of dummy_nodes.csv: \n",
    "\n",
    "    id,lat,lon,user,uid,version,changeset,timestamp\n",
    "    4826498152,0.0,0.0,dummy,-1,1,-1,2020-09-25T00:00:00Z\n",
    "    1763012935,0.0,0.0,dummy,-1,1,-1,2020-09-25T00:00:00Z\n",
    "    3823956308,0.0,0.0,dummy,-1,1,-1,2020-09-25T00:00:00Z\n",
    "    ...\n",
    "\n",
    "First 4 rows of dummy_nodes_tags.csv:  \n",
    "\n",
    "    id,key,value,type\n",
    "    4826498152,note,Way ref dummy node outside bounding box,regular\n",
    "    1763012935,note,Way ref dummy node outside bounding box,regular\n",
    "    3823956308,note,Way ref dummy node outside bounding box,regular\n",
    "    ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a style=\"font-size: small; font-style: italic\" href=#table-of-content>Back to table of content.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning operations <a id='cleaning-operations'></a>\n",
    "This chapter will document the cleaning process of weblinks and phone numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean weblinks (including nodes, ways and relations) <a id='clean-weblinks'></a>\n",
    "We'll identify weblinks in the OSM data set, standardize the weblink format and check if the weblink is valid/ working. On request a JSON file is written which provides a look-up table mapping the current to the corrected weblinks.\n",
    "\n",
    "The [OSM wiki for website tags](https://wiki.openstreetmap.org/wiki/Key%3Awebsite) provides information and recommendations for the use of *website* keys. A major principle is \"*Use as short a URL as possible*\". All weblinks should also conform with [RFC1738](https://tools.ietf.org/html/rfc1738#section-3.1). I use the URL schema found on Wikipedia ([URL encoding](https://de.wikipedia.org/wiki/URL-Encoding)) to show the different parts of a weblink.\n",
    "\n",
    "<p style=\"text-indent: 20%; font-weight: bold; font-size: small\">URL schema:</p>\n",
    "\n",
    "                            https://maxmuster:geheim@www.example.com:8080/index.html?p1=A&p2=B#ressource\n",
    "                            \\___/   \\_______/ \\____/ \\_____________/ \\__/\\_________/ \\_______/ \\_______/\n",
    "                              |         |       |           |         |       |          |         |\n",
    "                            schema     user  passphrase   host       port    path      query    fragment\n",
    "\n",
    "\n",
    "The following general steps are performed to clean the weblinks:\n",
    "- If possible use secure weblink (*https*)\n",
    "- Generally user/ password data shouldn't be part of a OSM *website*\n",
    "- OSM recommends to omit *www* to shorten the link\n",
    "- Providing the host part only is the most robust and long lasting usage\n",
    "- Port is optional\n",
    "- The path is also optional. If the path is broken/ doesn't work the weblink will be shortened to the host part\n",
    "- Querys should be omitted. I'll keep them if they work\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify weblinks\n",
    "According OSM weblinks can be found in *website*, *url*, *image* and *wikipedia* keys. I parse the tag keys of the OSM data to check the number of weblinks we find per key. In addition the found link is checked if it is a prober weblink. The python script [*audit_tags.py*](#list-of-files) contain the necessary functions to perform these tasks.\n",
    "\n",
    "```python\n",
    "weblinks, badlinks = weblinks_by_key(osm_file)\n",
    "```\n",
    "    Found weblinks in:\n",
    "      website: 322\n",
    "      wikipedia: 64\n",
    "      brand:wikipedia: 39\n",
    "      url: 12\n",
    "      contact:website: 167\n",
    "      image: 11\n",
    "      subject:wikipedia: 3\n",
    "      related:wikipedia: 2\n",
    "      removed:website: 1\n",
    "\n",
    "    Found bad links in:\n",
    "      wikipedia: 64\n",
    "      brand:wikipedia: 39\n",
    "      subject:wikipedia: 3\n",
    "      related:wikipedia: 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *wikipedia* keys don't contain proper weblinks as we can see in the following output,\n",
    "```python\n",
    "for i in range(10):\n",
    "    print(badlinks['wikipedia'][i])\n",
    "```\n",
    "    de:Kreis Pinneberg\n",
    "    de:Elmshorn\n",
    "    de:Flugplatz Ahrenlohe\n",
    "    de:Eiche von Barmstedt\n",
    "    de:Nikolaikirche (Elmshorn)\n",
    "    de:Hans Hachmann\n",
    "    de:Uetersener Wasserturm\n",
    "    de:Krückau\n",
    "    de:Krückau\n",
    "    de:Krückau\n",
    "\n",
    "but *image* keys do:\n",
    "```python\n",
    "for i in range(10):\n",
    "    print(weblinks['image'][i])\n",
    "```\n",
    "    https://upload.wikimedia.org/wikipedia/commons/c/c7/Stolperstein_Heinrich_Kastning.png\n",
    "    https://upload.wikimedia.org/wikipedia/commons/b/be/Stolperstein_Max_Wriedet.png\n",
    "    https://upload.wikimedia.org/wikipedia/commons/a/aa/Stolperstein_Reinhold_Jürgensen.png\n",
    "    https://upload.wikimedia.org/wikipedia/commons/9/94/Stolperstein_Max_Maack.png\n",
    "    https://upload.wikimedia.org/wikipedia/commons/f/f2/Stolperstein_Stanislaus_Pade.png\n",
    "    https://upload.wikimedia.org/wikipedia/commons/5/5d/Schleswig-Holstein%2C_Tornesch%2C_Naturdenkmal_12-01_NIK_2221.JPG\n",
    "    https://upload.wikimedia.org/wikipedia/commons/5/5d/Schleswig-Holstein%2C_Tornesch%2C_Naturdenkmal_12-01_NIK_2221.JPG\n",
    "    https://commons.wikimedia.org/wiki/File:Teehaus_Uetersen.JPG\n",
    "    http://www.zum-tannenbaum.de/files/bilder/hotel-pension.jpg\n",
    "    https://commons.wikimedia.org/wiki/File:Heidgraben_Green_Gables.jpg\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*website*, *contact:website*, *removed:website*, *url* and *image* keys contain weblinks. But are there any other keys containing weblinks? I will check other keys for weblinks using the *weblinks_by_value* function.\n",
    "\n",
    "```python\n",
    "weblinks_val = weblinks_by_value(osm_file)\n",
    "```\n",
    "    Found weblinks:\n",
    "      website: 322\n",
    "      openGeoDB:version: 6\n",
    "      email: 61\n",
    "      url: 12\n",
    "      note:de: 10\n",
    "      contact:website: 167\n",
    "      source: 40\n",
    "      contact:email: 20\n",
    "      name: 1\n",
    "      contact:facebook: 2\n",
    "      image: 11\n",
    "      internet_access:ssid: 1\n",
    "      removed:website: 1\n",
    "      operator: 1\n",
    "      note: 1\n",
    "      network: 12\n",
    "\n",
    "*openGeoDB:version*, *network*, *operator* and *name* do not contain proper links. The others do, like the key *source* as can be seen in the example below.\n",
    "\n",
    "    source:\n",
    "    http://www.gas-tankstellen.de\n",
    "    http://www.kindergarten-tornesch.de\n",
    "    http://www.kunst-im-auftrag.de/Projekte/Gezeiten-Projekte/Sturmflut-Dalben/sturmflut-dalben.html\n",
    "    http://www.heimathaus-tornesch.de/\n",
    "\n",
    "\n",
    " I also don't consider emails so we end up with the following keys containing weblinks:\n",
    "\n",
    "- *website*\n",
    "- *url*\n",
    "- *image*\n",
    "- *removed:website*\n",
    "- *contact:website*\n",
    "- *source*\n",
    "- *contact:facebook*\n",
    "- *internet_access:ssid* \n",
    "- *note*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to identify a weblink\n",
    "You might have asked yourself \"*how does he identify weblinks properly?*\". Well, this is done by defining the following regular expression:\n",
    "\n",
    "```python\n",
    "regex_weblink = re.compile(r'^(https?://)?(www\\.)?(.*\\.[a-zA-Z]{2,6})($|/{1}.*)$')\n",
    "```\n",
    "\n",
    "This regex identifies the *schema*, *host*, *www* and *path* part of an URL and splits the link into these four parts if existant. Let's consider a small example. Imagine we had the following url: *http://www.jugendpflege-uetersen.info/www/02_jugendzentrum/index.php?task=1*\n",
    "\n",
    "Matching and grouping our regex would result in:\n",
    "\n",
    "```python\n",
    "match = regex_weblink.match('http://www.jugendpflege-uetersen.info/www/02_jugendzentrum/index.php?task=1')\n",
    "for group in match.groups():\n",
    "    print(group)\n",
    "```\n",
    "    http://\n",
    "    www.\n",
    "    jugendpflege-uetersen.info\n",
    "    /www/02_jugendzentrum/index.php?task=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update weblinks\n",
    "The function *check_url* is used to check and if necessary to update the URL. On request a look-up table can be written to a JSON file for mapping the current (old) URL with the updated (new) one. \n",
    "\n",
    "Invoking this function updates the URLs and shows the follows statistics: \n",
    "```python\n",
    "lut, stats = weblink.check_url(osm_file, output=True, JSON_out='weblink_lut.all_d200923.1.JSON')\n",
    "```\n",
    "    Nbr insecure urls:  old/ 260   new/ 164\n",
    "    Nbr secure urls:    old/ 169   new/ 282\n",
    "    Nbr missing schemes:   17\n",
    "    Nbr modified links:     7\n",
    "    Nbr broken links:      63\n",
    "    Ndr of doublicates:    40\n",
    "    \n",
    "In the current OSM file there are 260 insecure (*http*) and 169 secure (*https*) links. After the update we have 164 insecure and 282 secure links. 17 links without schema are updated. 7 broken links could be fixed by updating the URL *path*. There are still 63 broken/ defect links in the data set. 40 links are doublicates. An exaple of the performed update is given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weblink [http://www.kruemet.de/unternehmen/filialen/filiale-elmshorn/](http://www.kruemet.de/unternehmen/filialen/filiale-elmshorn/) does not work. The function *check_url* updated the link by exchanging `http` by `https` and by removing `filiale-elmshorn/` from the URL *path*. In addition *www* is removed. The resulting link's now working: [https://kruemet.de/unternehmen/filialen/](https://kruemet.de/unternehmen/filialen/)\n",
    "\n",
    "**Attention:**  \n",
    "- Invoking this function on the complete OSM data set is pretty time consuming (~10 minutes) as the function invokes several hundret webpages\n",
    "- The results might not be exactly reproduceable as the validity of a page might have changed in the meantime\n",
    "- For this study the file *weblink_lut.all_d200923.1.JSON* is used to incorporate the weblink updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we check a weblink\n",
    "\n",
    "The url check is performed by the *requests* module. The short function that performs the job is shown below. It's part of the wrangle helper functions defined in [*wrangle_hlp.py*](#list-of-files). I've chosen the request type *head* to avoid loading the complete page content. Shorter *timeout*s than 4s increase the number of invalid URLs. It's important to allow redirection. Many (if not most) pages incorporate the *www* term in the *host* part of the URL which we've deleted to comply with the OSM recommendations.  \n",
    "\n",
    "```python\n",
    "import requests\n",
    "def check_weblink(link):\n",
    "    try:\n",
    "        r = requests.head(link, timeout=4, allow_redirects=True)        \n",
    "    except:\n",
    "        return False    \n",
    "    return r.status_code == requests.codes.ok\n",
    "```\n",
    "\n",
    "Whould be nice to know if there is an even easier and faster way to perfom the URL checks. If you have any suggestions leave a comment an github, please. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean phone numbers <a id='clean-phone-numbers'></a>\n",
    "\n",
    "#### General information\n",
    "\n",
    "The [OSM wiki for *phone* keys](https://wiki.openstreetmap.org/wiki/Key%3Aphone) provies information how phone numbers should look like in OSM data. Generally the phone number should comply with the international standard given in *ITU-T E.164* and should look like `+<country code> <area code> <local number>`. The image below provides some details refgarding the ITU standard.  \n",
    "<a id=\"InvestigatedArea_600x474\"></a>  \n",
    "![International_ITU-T_E.164-number_structure_for_geographic_areas](./images/International_ITU-T_E.164-number_structure_for_geographic_areas_400x266.PNG)\n",
    "\n",
    "<p style=\"text-indent: 20%; font-weight: bold; font-size: small\">Schema of phone numbers in acc. to <em>ITU-T E.164</em></p>  \n",
    "\n",
    "German area codes have been downloaded from [\"*Bundesnetzagentur*\"](https://www.bundesnetzagentur.de/DE/Sachgebiete/Telekommunikation/Unternehmen_Institutionen/Nummerierung/Rufnummern/ONRufnr/ON_Einteilung_ONB/ON_ONB_ONKz_ONBGrenzen_node.html) as csv file (*NVONB.INTERNET.20200916.ONB.csv*). In addition German mobile codes have been extracted from [Wikipedia](https://en.wikipedia.org/wiki/Telephone_numbers_in_Germany) and appended manually to this file. These ara codes are used to extract area codes from the OSM file and to check validity of the data. \n",
    "\n",
    "For auditing phone numbers we will consult the following keys:\n",
    "+ phone  \n",
    "+ phone2  \n",
    "+ fax\n",
    "+ contact:phone\n",
    "+ contact:fax\n",
    "+ communication:mobile\n",
    "\n",
    "I dismiss emergency codes in e.g. *regular:emergency_telephone_code* as these do not comply with ITU standard anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results of the phone updates\n",
    "The python script [*audit_phone.py*](#list-of-files) is used to audit and update phone numbers. Simply invoking *audit_phone* will perform the audits and updates. Below you can find some statistics and important details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "phnbr, problm, special, areas = audit_phone(osm_file, output=True)\n",
    "```\n",
    "    Nbr of phone numbers: 255\n",
    "    Nbr of unique area codes: 15\n",
    "    Nbr of numbers containing non-digit characters: 22\n",
    "    Nbr of problematic numbers (after cleaning): 0\n",
    "    Problematic numbers:\n",
    "    []\n",
    "\n",
    "There are 255 phone numbers in 15 unique area codes in the OSM file. 22 phone numbers did contail special characters. There is no number that couldn't be updated to ITU standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found area codes:  \n",
    "\n",
    "    Number of found area codes:  15\n",
    "    Area codes:\n",
    "    Area code:    151, area name: Deutsche Telekom (GSM/UMTS)\n",
    "    Area code:    152, area name: Vodafone D2 (GSM/UMTS)\n",
    "    Area code:    162, area name: Vodafone D2 (GSM/UMTS)\n",
    "    Area code:    176, area name: o2 Germany (GSM/UMTS)\n",
    "    Area code:    178, area name: E-Plus (merging into o2 Germany) (GSM/UMTS)\n",
    "    Area code:     32, area name: National subscriber numbers\n",
    "    Area code:     40, area name: Hamburg\n",
    "    Area code:   4101, area name: Pinneberg\n",
    "    Area code:   4120, area name: Ellerhoop\n",
    "    Area code:   4121, area name: Elmshorn\n",
    "    Area code:   4122, area name: Uetersen\n",
    "    Area code:   4123, area name: Barmstedt\n",
    "    Area code:   4126, area name: Horst Holstein\n",
    "    Area code:   4821, area name: Itzehoe\n",
    "    Area code:   4922, area name: Borkum\n",
    "    \n",
    "5 area codes are mobile phone codes. There is also a special code for natinal subscriber numbers. The remaining 9 codes are actual area codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some phone numbers initially containing special characters:\n",
    "\n",
    "    +49 4122 7107-70\n",
    "    +494121-4757577\n",
    "    +49 (4123) 92 17 93\n",
    "    +49 4121 645-274\n",
    "    +49 4122-9994713\n",
    "    +49 4121 4092-0\n",
    "    +49/4121/21773\n",
    "    +49 (4123) 68 40 24\n",
    "    +49 (4123) 68 400\n",
    "    +49 4121 643-0\n",
    "    +49 4121 57998-0\n",
    "    +49 4123 9290577;+49 4123 9222240\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find an extract of the updated phone numbers:\n",
    "\n",
    "    Updated phone numbers (before, after):\n",
    "    ...\n",
    "    ('+49 4122-9994713', '+49 4122 9994713')\n",
    "    ('+49 4122 7058', '+49 4122 7058')\n",
    "    ('+494122402132', '+49 4122 402132')\n",
    "    ('+49 4122 41776', '+49 4122 41776')\n",
    "    ('+49 4122 90421170', '+49 4122 90421170')\n",
    "    ('+49 4123 9369644', '+49 4123 9369644')\n",
    "    ('+4932221391378', '+49 32 221391378')\n",
    "    ('+49412244365', '+49 4122 44365')\n",
    "    ('+49 4121 103 83 30', '+49 4121 1038330')\n",
    "    ('+49 4121 750205', '+49 4121 750205')\n",
    "    ...\n",
    "\n",
    "Even multiple numbers are treated correctly:  \n",
    "    \n",
    "    ('+49 4123 9290577;+49 4123 9222240', '+49 4123 9290577;+49 4123 9222240')\n",
    "\n",
    "Ok, there is actually no update necessary as theses numbers are already ITU conform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some code explanation\n",
    "The function *update_phone* does the actual job of cleaning phone numbers. The tricky part in updating phone numbers is the identification of the area code as area codes in Germany have different length varying from 2 to 5 digits. The above mentioned csv file *NVONB.INTERNET.20200916.ONB.csv* containing all valid German area codes is used to identify the the area code within the *number*. This file is loaded and stored as dict *area_codes*. After stripping off the country code the remaining *number* consists of the *area* and the *subscriber* part. The code below is the actual engine to extract the area code. \n",
    "\n",
    "```python\n",
    "# Extract area code from number\n",
    "code_found = False\n",
    "for i in range(2,6):        # Area code consists of 2 to 5 digits\n",
    "    area = number[:i]       # area equals first i digits of number\n",
    "    # Iterate over area codes of length i and compare to area extracted from number\n",
    "    for code in (x for x in area_codes.keys() if len(x) == i):\n",
    "        if area == code:\n",
    "            code_found = True\n",
    "            subscriber = number.replace(area, '')  # Remaining string of number is subscriber part\n",
    "            break\n",
    "    if code_found == True: break\n",
    "# Return False if no area code found\n",
    "if code_found == False:\n",
    "    return False\n",
    "```\n",
    "\n",
    "This loop iterates over an area code (*area*) length between 2 and 5 digits. In the first run of the loop it assumes the area code to be of *i=2* digits length and compares the code with the valid codes of length *i=2* in the dict *area_codes*. If *area* is found in *area_codes* the remaining part of *number* is the *subscriber* part and the job is done. If *area* is not found the next iteration starts. The area code is assumed to be *i=3* digits and the new *area* is compared to all valid *area_codes* of length *i=3*. This procedure is repeated until a valid area code is found. Otherwise the function will return *False*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a style=\"font-size: small; font-style: italic\" href=#table-of-content>Back to table of content.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Database <a id='sql-database'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database creation <a id='database-creation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database schema used to create the SQL database (*osm.db*) can be seen below. The script [*export_OSM_data.py*](#list-of-files) is used create the database structure from OSM raw data and to validate and export each table (shown in the schema below) to a csv file. Subsequently the csv files are imported to the database. In addition the dummy nodes in *dummy_nodes.csv* and *dummy_nodes_tags.csv* mentioned in [Audit way nodes references](audit-way-nodes-references) are imported to the tables *nodes* and *nodes_tags*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is schema looks like:\n",
    "```sql\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database queries <a id='database-queries'></a>\n",
    "\n",
    "First I'd like to perform some spotchecks to see if special characterss we've encountered in auditing the OSM raw data have been properly transferred to the database. \n",
    "\n",
    "##### Problematic characters in tags   \n",
    "Node 1239947780 showed the following tag: \n",
    "```xml\n",
    "    <tag k=\"name\" v=\"BÜ 27 &quot;Wrangelpromenade&quot;\"/>\n",
    "```\n",
    "This is transferred to:  \n",
    "```sql\n",
    "SELECT * FROM nodes_tags\n",
    "WHERE id=1239947780 AND key='name';\n",
    "\n",
    "id        | key | value                   | type\n",
    "1239947780| name| BÜ 27 \"Wrangelpromenade\"| regular\n",
    "```\n",
    "\n",
    "  \n",
    "Node id 240036785 shows the following tag:\n",
    "```xml\n",
    "    <tag k=\"name:ru\" v=\"Эльмсхорн\"/>\n",
    "```\n",
    "This is correctly transferred to:\n",
    "```sql\n",
    "SELECT * FROM nodes_tags\n",
    "WHERE id=240036785 AND key='ru';\n",
    "\n",
    "id       | key| value    | type\n",
    "240036785| ru | Эльмсхорн| name\n",
    "```\n",
    "\n",
    "\n",
    "##### Problematic user names\n",
    "```sql\n",
    "SELECT DISTINCT user, uid from nodes\n",
    "WHERE user LIKE '%@%'\n",
    "UNION ALL\n",
    "SELECT DISTINCT user, uid from ways\n",
    "WHERE user REGEXP '[<>]';\n",
    "\n",
    "user    | uid\n",
    "nit@bse | 1041363\n",
    "@mmanuel| 6526984\n",
    "<don>   | 45059\n",
    "```\n",
    "That looks all pretty well. We'll continue with some database queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File sizes\n",
    "In the table below you can find a summary of file sizes for the various files used:\n",
    "\n",
    "File name | type | size in Mb\n",
    "--|--|--\n",
    "osm.db                         | SQL database      | 37\n",
    "GE_SH_PI_elmshorn_uetersen.osm | OSM raw/ xml file | 65\n",
    "ways.csv                       | csv file          | 3\n",
    "nodes.csv                      | csv file          | 20\n",
    "ways_tags.csv                  | csv file          | 6\n",
    "nodes_tags.csv                 | csv file          | 2\n",
    "ways_nodes.csv                 | csv file          | 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of unique users\n",
    "```sql\n",
    "SELECT count(*) unique_users FROM (\n",
    "SELECT uid, user FROM nodes n\n",
    "WHERE uid > 0\n",
    "UNION\n",
    "SELECT uid, user FROM ways w\n",
    "WHERE uid > 0 )\n",
    "\n",
    "unique_users\n",
    "570\n",
    "```\n",
    "There are 570 unique users in the database. Please bear in mind that we added dummy nodes to the database using *dummy* user with *uid*=-1. I will ignore the the dummy user, dummy nodes and dummy changeset for the following queries.\n",
    "\n",
    "##### Count number of nodes and ways (excluding dummy nodes)\n",
    "```sql\n",
    "SELECT type, number FROM (\n",
    "SELECT 'NODES' type, count(*) number FROM nodes\n",
    "WHERE uid>0\n",
    "UNION ALL\n",
    "SELECT 'WAYS' type, count(*) number FROM ways);\n",
    "\n",
    "type | number\n",
    "NODES| 252825\n",
    "WAYS | 51297\n",
    "```\n",
    "These are the exact numbers we've already seen in [Auditing OSM raw data](#auditing-osm-raw-data).\n",
    "\n",
    "#### Further queries\n",
    "First I'd like to see which user contributet the most to our investigated region. The following query shows the users changing the most elements (ways + nodes) and the number of performed changesets. The output is limited to the top ten contributers.\n",
    "\n",
    "```sql\n",
    "SELECT cs.uid, cs.user, count(*) nbr_changesets, sum(elements) FROM    \n",
    "(SELECT elms.changeset, elms.uid, elms.user, count(*) elements FROM\n",
    "(SELECT 'NODE' type, id, uid, user, version, changeset, timestamp FROM nodes\n",
    "WHERE changeset > 0\n",
    "UNION ALL\n",
    "SELECT 'WAY' type, id, uid, user, version, changeset, timestamp FROM ways\n",
    "WHERE changeset > 0) elms\n",
    "GROUP BY changeset) cs\n",
    "GROUP BY uid\n",
    "ORDER BY 4 DESC\n",
    "LIMIT 10\n",
    "\n",
    "uid    | user       | nbr_changesets| sum(elements)\n",
    "66904  | OSchlüter  | 674           | 135820\n",
    "1205786| westnordost| 477           | 48710\n",
    "1140155| Velorep    | 703           | 23412\n",
    "28234  | derandi    | 204           | 10111\n",
    "2969228| Críostaí   | 112           | 7369\n",
    "22646  | GeoMagician| 106           | 6684\n",
    "75623  | KTim       | 282           | 5753\n",
    "40397  | Zartbitter | 54            | 4159\n",
    "617520 | sundew     | 69            | 3832\n",
    "63375  | Divjo      | 38            | 3516\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm actually not content with the used database schema. The *user* and *uid* as well as the *changeset* can be found in the nodes and way tables. Also the tags could be combined to a single table. As I don't want to change the initial schema of our database I introduce a few *views* to make life more convienient.\n",
    "\n",
    "```sql\n",
    "CREATE VIEW vw_users AS\n",
    "    SELECT uid,\n",
    "           user\n",
    "      FROM nodes\n",
    "    UNION\n",
    "    SELECT uid,\n",
    "           user\n",
    "      FROM ways;\n",
    "\n",
    "CREATE VIEW vw_tags (\n",
    "    element,\n",
    "    id,\n",
    "    key,\n",
    "    value,\n",
    "    type\n",
    ")\n",
    "AS\n",
    "    SELECT 'NODE' element,\n",
    "           id,\n",
    "           key,\n",
    "           value,\n",
    "           type\n",
    "      FROM nodes_tags\n",
    "    UNION ALL\n",
    "    SELECT 'WAY' element,\n",
    "           id,\n",
    "           key,\n",
    "           value,\n",
    "           type\n",
    "      FROM ways_tags;\n",
    "```\n",
    "Above you can see the definition of the views *vw_tags* and *vw_users*. I also created the views: *vw_changesets*, *vw_nodes*, *vw_ways*. The following queries will use these views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to know the number of cafes in our region. The query below shows the element id, the name of the cafe as well as the city. \n",
    "\n",
    "```sql\n",
    "SELECT c.element, c.id, n.name, cy.city FROM ( \n",
    "SELECT element, id FROM vw_tags\n",
    "WHERE key = 'amenity' AND value = 'cafe') c\n",
    "LEFT OUTER JOIN (\n",
    "SELECT element, id, value name FROM vw_tags\n",
    "WHERE key='name') n\n",
    "ON c.id=n.id AND c.element=n.element\n",
    "LEFT OUTER JOIN (\n",
    "SELECT element, id, value city FROM vw_tags\n",
    "WHERE key='city') cy\n",
    "ON c.id=cy.id AND c.element=cy.element\n",
    "ORDER BY c.element, c.id \n",
    "\n",
    "element id          name                           city\n",
    "NODE    249752019   Café Langes Mühle              Uetersen\n",
    "NODE    270632008   Tommy's Eisbar                 Uetersen\n",
    "NODE    281690707   Café Galerie Schlossgefängnis  Barmstedt\n",
    "NODE    288893273   Uhlenhoff                      Kölln-Reisiek\n",
    "NODE    291902956   Rosenhof Kruse\t\n",
    "NODE    308946646   Die Pause\t\n",
    "NODE    385287257   Janny's Eis Cafe\t\n",
    "NODE    416915099   Café Kruschat                  Elmshorn\n",
    "NODE    416977988   Eis Cafe Südpol\t\n",
    "NODE    417459219   Eis Boutique\t\n",
    "NODE    490524213   \t\n",
    "NODE    1039130935  Jim Coffey\t\n",
    "NODE    1039131878  Diakonie-Cafe\t\n",
    "NODE    1039132160  Café Lykke\t\n",
    "NODE    1039237440  Eis Cafe Vittoria\t\n",
    "NODE    1314106748  Dielen-Café\t\n",
    "NODE    1314106802  Hof-Café\t\n",
    "NODE    2384582960  Keiser Eis\t\n",
    "NODE    2384613411  Bäckerei Eggers\t\n",
    "NODE    2833206591  Eiscafé Südpol                 Barmstedt\n",
    "NODE    2928174245  In Aller Munde\t\n",
    "NODE    2938199403  Ice Cafe Vittoria\t\n",
    "NODE    2944574838  \t\n",
    "NODE    2954932162  Conny's\t\n",
    "NODE    2954932182  Juli\t\n",
    "NODE    2955746486  Cafe el Pasha\t\n",
    "NODE    2955746531  Smokey's Shisha Bar\t\n",
    "NODE    2955746532  Stadt Cafe\t\n",
    "NODE    2955756272  Buongiorno Caffe               Elmshorn\n",
    "NODE    3669263093  Cafeteria (LMG)\t\n",
    "NODE    3669340850  Presse Café                    Uetersen\n",
    "NODE    3677249172  Café Ambiente\t\n",
    "NODE    3817744150  Klinikcafe\t\n",
    "NODE    3895894037  Kaffee Klatsch                 Barmstedt\n",
    "NODE    6651534704  Kolls\t\n",
    "NODE    7202870052  Cafe Billy's Morgenduft\t\n",
    "WAY     117856213   Monroe's                       Elmshorn\n",
    "WAY     511329270   Plantenhoff Cafe               Groß Nordende\n",
    "\n",
    "```\n",
    "There are 38 cafe in our region. 2 of them do not have a name :(.  \n",
    "\n",
    "There are some more queries you can find in [*db_queries.sql*](#list-of-files). Before we come to our last query I like to summarize as well as suggest some improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summay <a id='summary'></a>\n",
    "\n",
    "\n",
    "\n",
    "1) The database schema isn't the best. Incorporation of a *user* and *changeset* table would solve the issue of having users and changesets distibuted over different tables. The incorporated views are just a temporary solution. Queries would be much faster if we had actual tables instead of views. \n",
    "\n",
    "2) Looking for specific items in the database could become difficult when information is available at different places. We've seen this for phone numbers for example which can be found in numerus different keys like *phone*, *phone2*, *contact:phone* and *communication:mobile*. Beeing more restrictive could improve this situation but could also discourage users to contribute to OSM.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last query it's getting a bit *nesty*. We corrected phone numbers in [Clean phone numbers](#clean-phone-numbers). Now I want to now how often each area codes is used. This is done by the query below.\n",
    "```sql\n",
    "SELECT c.area_code, count(*) frequency FROM\n",
    "(SELECT *, substr(res, 1, end-1) area_code FROM\n",
    "(SELECT *, instr(res, ' ') end FROM\n",
    "(SELECT *, substr(value, pos+1) res FROM\n",
    "(SELECT *, instr(value, ' ') pos FROM vw_tags\n",
    "WHERE key IN ('phone', 'phone2', 'fax', 'fax', 'mobile') AND\n",
    "type IN ('contact', 'communication', 'regular'))))) c\n",
    "GROUP BY c.area_code\n",
    "ORDER BY frequency DESC\n",
    "\n",
    "area_code  frequency\n",
    "4121       155\n",
    "4122       43\n",
    "4123       33\n",
    "4126       6\n",
    "4101       4\n",
    "40         3\n",
    "4120       3\n",
    "151        1\n",
    "152        1\n",
    "162        1\n",
    "176        1\n",
    "178        1\n",
    "32         1\n",
    "4821       1\n",
    "4922       1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"font-size: small; font-style: italic\" href=#table-of-content>Back to table of content.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of references  <a id=\"list-of-references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list shows the consulted references and websites.  \n",
    "\n",
    "<dl>\n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://overpass-api.de/query_form.html'>https://overpass-api.de/query_form.html</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This weblink is used to extract the OSM raw data.</dd>\n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://wiki.openstreetmap.org/wiki/Key%3Awebsite'>https://wiki.openstreetmap.org/wiki/Key%3Awebsite</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This OSM wiki page provides information and recommendations regarding the usage of <em>website</em> tags.</dd>\n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://de.wikipedia.org/wiki/URL-Encoding'>https://de.wikipedia.org/wiki/URL-Encoding</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    URL encoding described on <em>Wikipedia</em>.</dd>    \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://tools.ietf.org/html/rfc1738#section-3'>https://tools.ietf.org/html/rfc1738#section-3</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   RFC1738 - Uniform Resource Locators (URL)</dd>      \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://wiki.openstreetmap.org/wiki/Key%3Aphone'>https://wiki.openstreetmap.org/wiki/Key%3Aphone</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   This OSM wiki page provides information and recommendations regarding the usage of <em>phone</em> tags.</dd>  \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://en.wikipedia.org/wiki/Telephone_numbers_in_Germany'>https://en.wikipedia.org/wiki/Telephone_numbers_in_Germany</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   Wikipedia resource providing information about German area codes for telefone numbers.</dd>     \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\"><a href='https://www.bundesnetzagentur.de/DE/Sachgebiete/Telekommunikation/Unternehmen_Institutionen/Nummerierung/Rufnummern/ONRufnr/ON_Einteilung_ONB/ON_ONB_ONKz_ONBGrenzen_node.html'>https://www.bundesnetzagentur.de/DE/Sachgebiete/Telekommunikation/Unternehmen_Institutionen/Nummerierung/Rufnummern/ONRufnr/ON_Einteilung_ONB/ON_ONB_ONKz_ONBGrenzen_node.html</a></dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    German official resource from <em>Bundesnetzagentur</em> for phone numbers and area codes. The csv file downloaded from this page is <em>NVONB.INTERNET.20200916.ONB.csv</em> (see <a href=#list-of-files>List of files</a>). There are regular updates on this page so that the current csv might not be the same.</dd> \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a style=\"font-size: small; font-style: italic\" href=#table-of-content>Back to table of content.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files  <a id=\"list-of-files\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list shows python scripts used to assess and clean the OSM raw data. In addition files containing data (csv, JSON) created during auditing and cleaning are mentioned as well. The python scripts can be found in the sub-directory `./scripts/`, data files in the sub-directory `./data/`.\n",
    "\n",
    "<br/>\n",
    "<dl> \n",
    "<dt style=\"font-style: italic; text-indent: 2%\">wrangle_hlp.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This script provides helping functions used throughout this investigation. The script is not intended to be run separately.</dd> \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">quick_parse_osm.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This script provides some functions to count the tags within the OSM xml file and to count the sub-tags included in <em>node</em>, <em>way</em> and <em>relation</em> tags.</dd> \n",
    "<br/> \n",
    "<dt style=\"font-style: italic; text-indent: 2%\">audit_nodes.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This script is used to audit all node fields. Each field has it's own auditing function. The fields <em>uid</em> and <em>user</em> as well as <em>version</em> and <em>changeset</em> are assessed within a single function.</dd> \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">audit_ways.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This script is used to audit all way attributes and node references. Each attribute has it's own auditing function. The attribute <em>uid</em> and <em>user</em> as well as <em>version</em> and <em>changeset</em> are assessed within a single function.</dd>\n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">audit_tags.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This script is used to audit node and way tags. It provides functions to audit the keys and the values of node tags. It also provides a function to check street names, post codes and country codes.</dd> \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">dummy_nodes.csv</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This file contains dummy nodes which are referenced by way node references. These nodes are missing in the current OSM data set as they are outside the investigated region.</dd>\n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">dummy_nodes_tags.csv</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "    This file contains dummy node tags for nodes outside the investigated region. A <em>note</em> tag is used to explain why these nodes (see <em>dummy_nodes.csv</em>) have been created.</dd> \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">audit_weblinks.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   This script is used to identify weblinks in the OSM data set, standardize the weblink format and check if the weblink is valid/ working. On request a JSON file is written which provides a look-up table mapping the current to the corrected weblinks.</dd> \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">weblink_lut.all_d200923.1.JSON</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   This file contains the mapping of the current URLs to the updates ones. It serves as a look-up table when writing the OSM data to csv files.</dd>     \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">NVONB.INTERNET.20200916.ONB.csv</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   This file contains German area codes extracted from <a href='https://www.bundesnetzagentur.de/DE/Sachgebiete/Telekommunikation/Unternehmen_Institutionen/Nummerierung/Rufnummern/ONRufnr/ON_Einteilung_ONB/ON_ONB_ONKz_ONBGrenzen_node.html'><em>Bundesnetzagentur</em></a> on 24.9.2020. At the end of the official list some mobile codes are appended manually.</dd>  \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">schema.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   Validation schema used by cerberus module</dd>     \n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">export_OSM_data.py</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   This python script is used to create the SQL database structure, extract the OSM raw data, perform cleaning operations for weblinks and phone numbers, validate the data and export the data/ tables to csv files.</dd>\n",
    "<br/>\n",
    "<dt style=\"font-style: italic; text-indent: 2%\">db_queries.sql</dt > \n",
    "<dd style=\"text-indent: 4%\">\n",
    "   Further SQL queries performed on osm.db</dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a style=\"font-size: small; font-style: italic\" href=#table-of-content>Back to table of content.</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
